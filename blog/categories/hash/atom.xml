<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hash | 技术小黑屋]]></title>
  <link href="http://droidyue.com/blog/categories/hash/atom.xml" rel="self"/>
  <link href="http://droidyue.com/"/>
  <updated>2014-12-13T10:36:12+08:00</updated>
  <id>http://droidyue.com/</id>
  <author>
    <name><![CDATA[androidyue]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Get MD5 Hash of Big Files]]></title>
    <link href="http://droidyue.com/blog/2013/10/07/get-md5-hash-of-big-files/"/>
    <updated>2013-10-07T10:16:00+08:00</updated>
    <id>http://droidyue.com/blog/2013/10/07/get-md5-hash-of-big-files</id>
    <content type="html"><![CDATA[<p>Recently I have been dealing with files and I need to get md5 hash of all kinds of files;Some are small and some are big.<br/>
For the small files I use this method to get md5 hash value.
```python
def getFileMd5(filename):</p>

<pre><code>file = open(filename, 'rb') 
m = md5()
m.update(file.read())
file.close()
result =  m.hexdigest()
return result
</code></pre>

<p><code>
However for calculating md5 hash value of big files,the above method will be very Less Efficient.For Big files I use the following method(It's acquired from stackoverflow)
</code>python
def getBigFileMd5(filename, block_size=2**20):</p>

<pre><code>f = open(filename, 'rb')
m = md5()
while True:
    data = f.read(block_size)
    if not data:
        break
    m.update(data)
f.close()
return m.hexdigest()
</code></pre>

<p>```
And I did a test.The cost of Getting md5 hash of a Big file(size:10.7 GiB; 11,455,512,109 bytes) is 213.447s.And I think it&rsquo;s OK.</p>
]]></content>
  </entry>
  
</feed>
